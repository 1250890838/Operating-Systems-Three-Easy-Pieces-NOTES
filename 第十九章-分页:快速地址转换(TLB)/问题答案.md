1. 为了计时,可能需要一个计时器,(例如`gettimeofday`)。这种计时器的精度如何?操作要花多少时间,才能让你对它精确计时?(这有助于确定需要循环多少次,反复访问内存页,才能对它成功计时。)

`gettimeofday` 精度为微秒级,成本也为微秒级([参考](https://blog.csdn.net/russell_tao/article/details/7185588))
`rdtsc`指令,成本与精确度都是纳秒级(取决于cpu使用周期)

测试方法参考[How to Benchmark Code Execution Times on Intel® IA-32 and IA-64 Instruction Set Architectures](https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/ia-32-ia-64-benchmark-code-execution-paper.pdf)


2. 写一个程序,命名为tlb.c,大体测算一下每个页的平均访问时间。程序的输入参数有:页的数目和尝试的次数。

```
gcc tlb.c && ./a.out -l 15 -p 15
```
l参数为测试次数,p参数为页数

3. 用你喜欢的脚本语言(csh、 Python等写一段脚本来运行这个程序,当访问页面从1增长到几千,也许每次迭代都乘2。在不同的机器上运行这段脚本,同时收集相应数据。需要试多少次才能获得可信的测量结果?

```
python3 run.py
```

4. 接下来,将结果绘图,类似于上图。可以用 ploticus这样的好工具画图。可视化使数据更容易理解,你认为是什么原因?

```
bash run.sh
```

![图片](./4.png))

5. 要注意编译器优化带来的影响。编译器做各种聪明的事情,包括优化掉循环,如果循环中增加的变量后续没有使用。如何确保编译器不优化掉你写的TLB大小测算程序的主循环?

> gcc选项-O启用不同级别的优化。使用-O0(默认)禁用它们。-O3是最高级别的优化。

6. 还有一个需要注意的地方,今天的计算机系统大多有多个CPU,每个CPU当然有自己的TLB结构。为了得到准确的测量数据我们需要只在一个CPU上运行程序,避免调度器把进程从一个CPU调度到另一个去运行。如何做到?(提示:在 Google上搜索“ pinningthread”相关的信息)如果没有这样做,代码从一个CPU移到了另一个,会发生什么情况?

```
pthread_setaffinity_np
```
切换cpu时成本增加,但时使用新的小TLB使速度变快

7. 另一个可能发生的问题与初始化有关。如果在访问数组a之前没有初始化,第一次访问将非常耗时,由于初始访问开销,比如要求置0。这会影响你的代码及其计时吗?如何抵消这些潜在的开销?

不会,没有初始化,且计时器不会记录初始化的时间